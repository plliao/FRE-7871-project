\documentclass[sigconf]{acmart}

\usepackage{url}

\newcommand{\ts}{\textsuperscript}

\begin{document}

\title{Reproduce Stock Price Prediction Tasks with News Analysis}
\subtitle{NYU FRE-7871 Final Project Proposal}

\author{Pei-Lun Liao}
\affiliation{%
  \institution{New York University}
}
\email{pll273@nyu.edu}

\begin{abstract}
In this project \footnote{The source code was released on https://github.com/plliao/FRE-7871-project}, selected related works will be reproduced with different financial news dataset. The goal is to understand techniques and challenges in stock price prediction.
\end{abstract}
\maketitle

\section{Introduction}
Nowadays, traders apply machine learning technique to extract information from financial news data. The information can be used to boost the accuracy of stock price
prediction \cite{AZFinText, Ding2014, Ding2015}. The project aims to reproduce selected related works \cite{AZFinText}. Hence, people can learn the challenge and technique
that are crucial in stock price prediction.

\section{Approaches}
\subsection{Data}
Data quality is crucial to make the accurate prediction \cite{stanford}. Unfortunately, it is not easy to find a publicly available financial news dataset. For example, the public financial
news dataset published in \cite{Ding2014} was currently unavailable due to license issue \cite{fn}. Another corpus available online requires membership and subscription fee \cite{nyt}.
Also, it is not practical to crawl dataset from business news media in this one month project. Most of the time will be spent on data collecting, cleaning, and processing instead of
understanding the technique that works for stock price prediction.

\subsubsection{Webhose.io and Yahoo! Finance}
Fortunately, an available financial news dataset was found on Webhose.io \cite{data}. The data was crawled from the Internet from July to October in 2015. 47,851 news articles were collected
in machine-readable format. However, there is no paper shows the dataset could help stock price prediction. News articles from Webhose.io come globally. To make sure the articles related to the
America stock index, I filtered out the news from other countries and the articles with words number less than 100, and only pick the articles which contain the word "finance" \cite{stanford}.  

End of date S \& P 500 index from July to October in 2015 was downloaded from Yahoo! Finance \cite{yf} . However, we can't find the S \& P index by minute and corresponding 500 stock price by minute.
Therefore, we could not have the S \& P 500 index by minute to predict next 20 minutes stock index as the experiment in AZFinText. What we can do is to predict the close time price and to see if we can improve
the return rate. Since we only have a datum per day, it turns out only 76 EOD data was left. It may hurt the performance of a machine learning model.

\subsubsection{Reuters and IEX API}
Due to the quality issue in Webhose.io dataset, we need to find another way to do the experiment. Hence, we crawled S \& P 500 stock price from IEX API \cite{IEX} and business news articles from 
Reuters \cite{reuters} in the period between July 2\ts{nd} 2018 to September 30\ts{th} 2018. Due to the limited available open stock price data, I can only collect 3 months dataset. Also, the number
of articles crawled is unknown, and data quality is not guaranteed as well. We will find S \& P 500 company names and tickers in an article and map the article to a specific stock. Our goal is predicting the
next 20 minutes stock price as described in AZFixText.

\subsection{Plan}
\subsubsection{Stage 1: bag-of-words}
	The goal in stage 1 is reproducing the experiment result in AZFinText system \cite{AZFinText}. AZFinText represented article in the bag-of-words with only proper nouns. The
	dataset were Yahoo Finance news articles and the S \& P 500 index. Reproducing the experiment in AZFinText can prove that the data from Webhose.io also works for stock
	price prediction. Moreover, we examined whether only using proper nouns helps the performance.

	The experiment setup will follow the AZFinText paper. We will represent articles in the bag-of-words and use SVR to predict stock price. Finally, we evaluate the result by the rate of return.
	The strategy is buying the stock as the predicted price is greater than or equal to 1\% movement from the stock price at the time the article was released. Then, sell the stock after 20 minutes.
	In stage 1, we will have four different results to compare.
	\\
	\begin{itemize}
		\item 1. Stock price
		\item 2. Stock price + News in bag-of-words
		\item 3. Stock price + News in bag-of-words without stopwords
		\item 4. Stock price + News in bag-of-words with only proper nouns
	\end{itemize}
	The expected performance will be 4 > 3 > 2 > 1 as the paper described.

\subsubsection{Stage 2: word and sentence representation}
	In this stage, We are curious how modern deep learning techniques like word2vec, seq2seq, and CNN-LSTM language models could help improve the performance.  
	\begin{itemize}
		\item 5. Stock price + News feature in average word2vec \cite{word2vec1, word2vec2}
		\item 6. Stock price + News feature in Skip-Thought vector\cite{skip}
		\item 7. Stock price + News feature in CNN-LSTM encoding \cite{CNN2RNN} (may try to find pre-trained model)
	\end{itemize}
	The performance in stage 2 should be better than the one in stage 1.

\section{Experiment}
\subsection{Data processing}
The articles published before the stock close time are collected. We selected the last 5 articles per day with at least 100 words each article. If the article doesn't contain the word "finance" or the article does
not come from America, we filter them out. After that, we remove stop words, punctuation, number, low-frequency words, and high-frequency words in the article. The threshold of the low and high frequency
are 10 and 100. We merge 5 articles bag-of-words into 1 article and normalize it with L2 norm.  For the price features, I use the previous 5 days open and close price difference as features. The idea is trying to learn
the trend by days. Since we only have 76 days, it is not likely to reproduce the result in AZFinText \cite{AZFinText}.
\subsection{Tasks}
We have three different tasks. The classification task is to predict if the close price goes up. The regression problem is to predict the price difference. The final one is applying a simple strategy to see what we can
gain. The strategy is trading as the predicted price difference is positive. We use logistic regression as our classification model, ridge regression for regression task.
\begin{itemize}
	\item Classification task: close price goes up or down
	\item Regression task: close and open price difference
	\item Trading strategy task: compute return using the prediction price
\end{itemize} 
The reason for not applying the SVR model as in AZFinText is that SVR always picks the same support vectors among different features. The insufficient data may cause it.
The result is not comparable among them. Also, the reason not using the same strategy as in AZFinText is for all models we could not have 1 \% return. Hence, we have 0 return rate for all models and features.
So the result is not comparable as well. Hence, we start with a more straightforward strategy.

\subsection{Evaluation}
Since the data is time series, we can't apply standard K-fold cross-validation to our model because we can have the forward bias. Hence, we use 5 fold time series cross-validation instead. We built word
dictionary and scaled our features only on the observable dataset. Then, we predict the dataset on the next fold. If there are new words in the article, we ignore them.
We evaluate accuracy for the classification task, mean square error for the regression task and return rate in percentage for trading strategy task.

\section{Result}
\subsection{Stage 1: bag-of-words}
We evaluate the performance with different features. Table \ref{table:index} shows the result for using only price index features. Table \ref{table:stopwords} and table \ref{table:nostopwords} shows the result
for using price index features and text features. The result shows it hard to predict the future with insufficient data, and the model is not likely to make money with less than 1 \% return rate. Sometimes we overfit like the case in
the period of 09/04 - 09/22 and 09/23 - 10/08 in table \ref{table:index}. However, generally speaking, the text features could help the performance a little bit like the table \ref{table:compare} shows.
Nonetheless, we only have 76 data. The result was not convincible.

We also find the important terms in our article by checking the weight of coefficient in our models in table \ref{table:importance}. It gives us the explanation that why the performance between the text features with and
without stopwords is similar because they have the similar vital terms in their models. However, the terms do not make much sense to me. The found words are general and not obvious to say it can make the impact
on the stock price. 

\begin{table*}
\centering
\begin{tabular}{ccccc}
Prediction date & Word size & Classification (acc) & Regression (MSE) & Trading (return \%) \\
07/31 - 08/17   & -         & 0.687 / 0.666        & 119.07 / 206.77  & 0.172 / -0.037      \\
08/18 - 09/03   & -         & 0.678 / 0.833        & 143.70 / 1168.6  & 0.096 / 0.706       \\
09/04 - 09/22   & -         & 0.7 / 0.416          & 376.43 / 537.69  & 0.463 / -0.180      \\
09/23 - 10/08   & -         & 0.653 / 0.5          & 393.41 / 643.12  & 0.352/ 0.113        \\
10/09 - 10/26   & -         & 0.593 / 0.833        & 429.95 /  152.88 & 0.302 / 0.449      
\end{tabular}
\caption{Experiment result with S \& P 500 index features}
\label{table:index}
\end{table*}

\begin{table*}
\centering
\begin{tabular}{ccccc}
Prediction date & Word size & Classification (acc) & Regression (MSE) & Trading (return \%) \\
07/31 - 08/17   & 680       & 0.875 / 0.666        & 55.419 / 215.57  & 0.475 /  -0.018     \\
08/18 - 09/03   & 1207      & 0.785 / 0.833        & 64.944 / 1188.86 & 0.521 /  0.896      \\
09/04 - 09/22   & 1583      & 0.85 / 0.416         & 133.75 / 507.64  & 0.900 /  0.213      \\
09/23 - 10/08   & 1852      & 0.865 / 0.5          & 128.37 / 695.36  & 0.937 / 0.113       \\
10/09 - 10/26   & 2188      & 0.765 / 0.75         & 143.76  / 132.99 & 0.904 / 0.502      
\end{tabular}
\caption{Experiment result with S \& P 500 index features and bag-of-words text features with stopwords}
\label{table:stopwords}
\end{table*}

\begin{table*}
\centering
\begin{tabular}{ccccc}
Prediction date & Word size & Classification (acc) & Regression (MSE) & Trading (return \%) \\
07/31 - 08/17   & 520       & 0.875 / 0.666        & 47.378 / 226.24  & 0.438 / -0.018      \\
08/18 - 09/03   & 1043      & 0.821 / 0.833        & 62.046 / 1166.6  & 0.556 / 0.896       \\
09/04 - 09/22   & 1427      & 0.825 / 0.416        & 132.15 / 517.17  & 0.900 / 0.213       \\
09/23 - 10/08   & 1700      & 0.865 / 0.5          & 128.08 / 694.24  & 0.937 / 0.113       \\
10/09 - 10/26   & 2055      & 0.765 / 0.75         & 143.42 / 126.87  & 0.865 / 0.502      
\end{tabular}
\caption{Experiment result with S \& P 500 index features and bag-of-words text features without stopwords}
\label{table:nostopwords}
\end{table*}

\begin{table*}
\centering
\begin{tabular}{cccc}
10/09 - 10/26          & Classification (acc) & Regression (MSE) & Trading (return \%) \\
Index feature          & 0.593 / 0.833        & 429.95 /  152.88 & 0.302 / 0.449       \\
Text with stopwords    & 0.765 / 0.75         & 143.76  / 132.99 & 0.904 / 0.502       \\
Text without stopwords & 0.765 / 0.75         & 143.42 / 126.87  & 0.865 / 0.502      
\end{tabular}
\caption{Experiment result with different features on the last fold of validation}
\label{table:compare}
\end{table*}

\begin{table*}
\centering
\begin{tabular}{cccc}
Model               & stopwords & positive terms                                                                             & negative terms                                                                      \\
Logistic Regression & V         & {[}'ahead' 'equity' 'half' 'income' 'lse'{]}               & {[}'cents' 'crude' 'customers' 'debt' 'dollar'{]} \\
Logistic Regression &           & {[}'ahead' 'equity' 'half' 'income' 'lse'{]}               & {[}'cents' 'crude' 'customers' 'dollar' 'japanese'{]} \\
Ridge Regression    & V         & {[}'acquire' 'credit' 'deadline' 'half' 'index'{]} & {[}'against' 'care' 'court' 'dollar' 'ecb'{]}   \\
Ridge Regression    &           & {[}'acquire' 'credit' 'deadline' 'half' 'index'{]} & {[}'care' 'court' 'dollar' 'ecb' 'japanese'{]}  \\
Naive Bayes         & V         & {[}'information' 'oil' 'per' 'technology' 'your'{]}                                        & {[}'debt' 'oil' 'per' 'rate' 'your'{]}                                              \\
Naive Bayes         &           & {[}'information', 'lse', 'oil', 'rate', 'technology'{]}                                    & {[}'china', 'debt', 'dollar', 'oil', 'rate'{]}                                     
\end{tabular}
\caption{Terms importance analysis}
\label{table:importance}
\end{table*}

\section{Future work}
The future works are cleaning the Reuters dataset and do the same experiment again. Also, I would like to try the technique described in AZFinText
by only selecting noun words in an article as features and begin the stage 2 in my plan.

\bibliographystyle{ACM-Reference-Format}
\bibliography{citation} 
\end{document}
